{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUWCAY5opP87"
   },
   "source": [
    "<p style=\"align: center;\"><img align=center src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\"  width=500 height=400></p>\n",
    "\n",
    "<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sEkVD5qHpP89"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wj5MrpmRpP89"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Элементы теории оптимизации. Производные и частные производные.</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O1xIGzO5pP8-"
   },
   "source": [
    "<p style=\"text-align: center;\">(На основе https://github.com/romasoletskyi/Machine-Learning-Course)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ODaZDX75pP8-"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Приращение линейной функции</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tldAx431pP8_"
   },
   "source": [
    "Давайте рассмотрим линейную функцию $y=kx+b$ и построим график: <br>  \n",
    "\n",
    "![source: Wikipedia](https://upload.wikimedia.org/wikipedia/commons/c/c1/Wiki_slope_in_2d.svg) <br>  \n",
    "\n",
    "Введём понятие **приращения** функции в точке $(x, y)$ как отношение вертикального изменения (измненеия функции по вертикали) $\\Delta y$ к горизонтальному изменению $\\Delta x$ и вычислим приращение для линейной функции:  \n",
    "\n",
    "$$приращение (\"slope\")=\\frac{\\Delta y}{\\Delta x}=\\frac{y_2-y_1}{x_2-x_1}=\\frac{kx_2+b-kx_1-b}{x_2-x_1}=k\\frac{x_2-x_1}{x_2-x_1}=k$$  \n",
    "\n",
    "Видим, что приращение в точке у прямой не зависит от $x$ и $\\Delta x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ObcFoC9JpP9A"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Приращение произвольной функции</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dNfWm09WpP9A"
   },
   "source": [
    "Но что, если функция не линейная, а произвольная $f(x)$?  \n",
    "В таком случае просто нарисуем **касательную ** в точке, в которой ищем приращение, и будем смотреть уже на приращение касательной. Так как касательная - это прямая, мы уже знаем, какое у неё приращение (см. выше).\n",
    "![source: Wikipedia](https://upload.wikimedia.org/wikipedia/commons/d/d2/Tangent-calculus.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "137KHuBjpP9B"
   },
   "source": [
    "Имея граик функции мы, конечно, можем нарисовать касательную в точке. Но часто функции заданы аналитически, и хочется уметь сразу быстро получать формулу для приращения функциии в точке. Тут на помощь приходит **производная**.  Давайте посмотрим на определение производной его с нашим понятием приращения:  \n",
    "\n",
    "$$f'(x) = \\lim_{\\Delta x \\to 0}\\frac{\\Delta y}{\\Delta x} = \\lim_{\\Delta x \\to 0}\\frac{f(x + \\Delta x) - f(x)}{\\Delta x}$$  \n",
    "\n",
    "То есть по сути, значение производной функции в точке - это и есть приращение функции, если мы стремим длину отрезка $\\Delta x$ к нулю."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7YksIkmlpP9C"
   },
   "source": [
    "Посомтрим на интерактивное демо, демонстрирующее стремление $\\Delta x$ к нулю (*в Google Colab работать не будет!*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v9rhGojJpP9D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The birdseye extension is already loaded. To reload it, use:\n",
      "  %reload_ext birdseye\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from time import sleep\n",
    "from math import exp\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%load_ext birdseye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R4DbxljwpP9F"
   },
   "outputs": [],
   "source": [
    "#!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJ_xbrHXpP9I"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea722d240376496580d60f329c5a8908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='lg_z', max=4.0, min=-0.5), Output()), _dom_classes=(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(lg_z=(-0.5,4.0,0.1))\n",
    "def f(lg_z=1.0):\n",
    "    z = 10 ** lg_z\n",
    "    x_min = 1.5 - 6/z\n",
    "    x_max = 1.5 + 6/z\n",
    "    l_min = 1.5 - 4/z\n",
    "    l_max = 1.5 + 4/z\n",
    "    xstep = (x_max - x_min)/100\n",
    "    lstep = (l_max - l_min)/100\n",
    "    \n",
    "    x = np.arange(x_min, x_max, xstep)\n",
    "    \n",
    "    plt.plot(x, np.sin(x), '-b')     \n",
    "\n",
    "    plt.plot((l_min,l_max), (np.sin(l_min), np.sin(l_max)), '-r')\n",
    "    plt.plot((l_min,l_max), (np.sin(l_min), np.sin(l_min)), '-r')\n",
    "    plt.plot((l_max,l_max), (np.sin(l_min), np.sin(l_max)), '-r')\n",
    "    \n",
    "    yax = plt.ylim()    \n",
    "    \n",
    "    plt.text(l_max + 0.1/z, (np.sin(l_min) + np.sin(l_max)) / 2, \"$\\Delta y$\")\n",
    "    plt.text((l_min + l_max)/2, np.sin(l_min) - (yax[1]-yax[0]) / 20, \"$\\Delta x$\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print('slope =', (np.sin(l_max) - np.sin(l_min)) / (l_max - l_min))\n",
    "    print('delta x =', xstep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8CYa2CRpP9N"
   },
   "source": [
    "Видим, что при уменьшении отрезка $\\Delta x$, значение приращения стабилизируется (перестаёт изменяться). Это число и есть приращение функции в точке, равное проиводной функции в точке. Производную функции $f(x)$ в точке x обознают как $f'(x)$ или как $\\frac{d}{dx}(f(x))$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VMwBqnhVpP9N"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Пример вычисления проиводной</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JwlAAsznpP9P"
   },
   "source": [
    "Возьмём производную по определению:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6R_rnMsqpP9P"
   },
   "source": [
    "1. $f(x)=x$  \n",
    "\n",
    "$$\\frac{\\Delta y}{\\Delta x}=\\frac{x+\\Delta x-x}{\\Delta x}=1\\Rightarrow \\mathbf{\\frac{d}{dx}(x)=1}$$  \n",
    "\n",
    "2. $f(x)=x^2$  \n",
    "\n",
    "$$\\frac{\\Delta y}{\\Delta x}=\\frac{(x+\\Delta x)^2-x^2}{\\Delta x}=\\frac{x^2+2x\\Delta x+\\Delta x^2-x^2}{\\Delta x}=2x+\\Delta x\\rightarrow 2x (\\Delta x\\rightarrow 0)\\Rightarrow \\mathbf{\\frac{d}{dx}(x^2)=2x}$$  \n",
    "    \n",
    "3. В общем случае для степенной функции $f(x)=x^n$ формула будет такой:  \n",
    "\n",
    "$$\\mathbf{\\frac{d}{dx}(x^n)=nx^{n-1}}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KP4jUOaqpP9P"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Правила вычисления проиводной</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8fb0go1lpP9Q"
   },
   "source": [
    "Выпишет правила *дифференцирования*:  \n",
    "\n",
    "1). Если $f(x)$ - константа, то её производная (приращение) 0:  \n",
    "\n",
    "$$(C)' = 0$$\n",
    "\n",
    "2). Производная суммы функций - это сумма производных:  \n",
    "\n",
    "$$(f(x) + g(x))' = f'(x) + g'(x)$$\n",
    "\n",
    "3). Производная разности - разность производных:  \n",
    "\n",
    "$$(f(x) - g(x))' = f'(x) - g'(x)$$\n",
    "\n",
    "4). Производная произведения функций:  \n",
    "\n",
    "$$(f(x)g(x))' = f'(x)g(x) + f(x)g'(x)$$\n",
    "\n",
    "5). Производная частного:  \n",
    "\n",
    "$$\\left(\\frac{f(x)}{g(x)}\\right)'=\\frac{f'(x)g(x)-g'(x)f(x)}{g^2(x)}$$\n",
    "\n",
    "6). Производная сложной функции (\"правило цепочки\", \"chain rule\"):  \n",
    "\n",
    "$$(f(g(x)))'=f'(g(x))g'(x)$$\n",
    "\n",
    "Можно записать ещё так:  \n",
    "\n",
    "$$\\frac{d}{dx}(f(g(x)))=\\frac{df}{dg}\\frac{dg}{dx}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZFTReW5ppP9R"
   },
   "source": [
    "**Примеры**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "grjqr2h4pP9R"
   },
   "source": [
    "* Вычислим производную функции $$f(x) = \\frac{x^2}{cos(x)} + 100$$:  \n",
    "\n",
    "$$f'(x) = \\left(\\frac{x^2}{cos(x)}+100\\right)' = \\left(\\frac{x^2}{cos(x)}\\right)' + (100)' = \\frac{(2x)\\cos(x) - x^2(-\\sin(x))}{cos^2(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TSqCaSSYpP9T"
   },
   "source": [
    "* Вычислим производную функции $$f(x) = tg(x)$$:  \n",
    "\n",
    "$$f'(x) = \\left(tg(x)\\right)' = \\left(\\frac{\\sin(x)}{\\cos(x)}\\right)' = \\frac{\\cos(x)\\cos(x) - \\sin(x)(-\\sin(x))}{cos^2(x)} = \\frac{1}{cos^2(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FXP_YETzpP9T"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Частные производные</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aJDwZBCZpP9T"
   },
   "source": [
    "Когда мы имеем функци многих переменных, её уже сложнее представить себе в виде рисунка (в случае более 3-х переменных это действительно не всем дано). ОДнако формальные правила взятия производной у таких функций созраняются. Они в точности совпадают с тоеми, которые рассмотрены выше для функции одной переменной.  \n",
    "\n",
    "Итак, правило взятия частной производной функции мнгих переменных:  \n",
    "1). Пусть $f(\\overline{x}) = f(x_1, x_2, .., x_n)$ - функция многих переменных;  \n",
    "2). Частная проиводная по $x_i$ это функции - это производная по x_i, считая все остальные переменные **константами**. \n",
    "\n",
    "Более математично:  \n",
    "\n",
    "Частная производная функции $f(x_1,x_2,...,x_n)$ по $x_i$ равна  \n",
    "\n",
    "$$\\frac{\\partial f(x_1,x_2,...,x_n)}{\\partial x_i}=\\frac{df_{x_1,...,x_{i-1},x_{i+1},...x_n}(x_i)}{dx_i}$$  \n",
    "\n",
    "где $f_{x_1,...,x_{i-1},x_{i+1},...x_n}(x_i)$ означает, что переменные $x_1,...,x_{i-1},x_{i+1},...x_n$ - это фиксированные значения, и с ними нужно обращаться как с константами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3uPIkZ-wpP9U"
   },
   "source": [
    "**Примеры**:   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aodpt9VppP9V"
   },
   "source": [
    "* Найдём частные производные функции $f(x, y) = -x^7 + (y - 2)^2 + 140$ по $x$ и по $y$:  \n",
    "\n",
    "$$f_x'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{x}} = -7x^6$$  \n",
    "$$f_y'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{y}} = 2(y - 2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3pRitR-YpP9W"
   },
   "source": [
    "* Найдём частные производные функции $f(x, y, z) = \\sin(x)\\cos(y)tg(z)$ по $x$, по $y$ и по $z$:  \n",
    "\n",
    "$$f_x'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{x}} = \\cos(x)\\cos(y)tg(z)$$  \n",
    "$$f_y'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{y}} = \\sin(x)(-\\sin(y))tg(z)$$\n",
    "$$f_z'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{y}} = \\frac{\\sin(x)\\cos(y)}{\\cos^2{z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nrmPlYyrpP9X"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Градиентный спуск</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SeD8U4CApP9X"
   },
   "source": [
    "**Градиентом** функции $f(\\overline{x})$, где $\\overline{x} \\in \\mathbb{R^n}$, то есть $\\overline{x} = (x_1, x_2, .., x_n)$, называется вектор из частных производных функции $f(\\overline{x})$:  \n",
    "\n",
    "$$grad(f) = \\nabla f(\\overline{x}) = \\left(\\frac{\\partial{f(\\overline{x})}}{\\partial{x_1}}, \\frac{\\partial{f(\\overline{x})}}{\\partial{x_2}}, .., \\frac{\\partial{f(\\overline{x})}}{\\partial{x_n}}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DcjYCHOepP9Y"
   },
   "source": [
    "Есть функция $f(x)$. Хотим найти аргумент, при котором она даёт минимум.\n",
    "\n",
    "Алгоритм градиентного спуска:  \n",
    "1. $x^0$ - начальное значение (обычно берётся просто из разумных соображений или случайное);  \n",
    "2. $x^i = x^{i-1} - \\alpha \\nabla f(x^{i-1})$, где $\\nabla f(x^{i-1})$ - это градиент функции $f$, в который подставлено значение $x^{i-1}$;\n",
    "3. Выполнять пункт 2, пока не выполнится условие остановки: $||x^{i} - x^{i-1}|| < eps$, где $||x^{i} - x^{i-1}|| = \\sqrt{(x_1^i - x_1^{i-1})^2 + .. + (x_n^i - x_n^{i-1})^2}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zX1miuQ0pP9Z"
   },
   "source": [
    "**Примеры:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t1M6agxdpP9Z"
   },
   "source": [
    "* *Пример 1*: Посчитаем формулу градиентного спуска для функции $f(x) = 10x^2$:   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1WMJRqDRpP9a"
   },
   "source": [
    "$x^i = x^{i-1} - \\alpha \\nabla f(x^{i-1}) = x^{i-1} - \\alpha f'(x^{i-1}) = x^{i-1} - \\alpha (20x^{i-1}) = x^{i-1} - 20\\alpha x^{i-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EqjopRZVpP9b"
   },
   "source": [
    "Имея эту формулу, напишем код градиентного спуска для функции $f(x) = 10x^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "evLahkyIpP9c"
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 10 * x**2\n",
    "\n",
    "def gradient_descent(alpha=0.01, eps=0.001):\n",
    "    x_pred = 0  # начальная инициализация\n",
    "    x = 50  # начальная инициализация\n",
    "    for _ in tqdm_notebook(range(100000), desc='1st loop'):\n",
    "        if np.sum((x - x_pred)**2) < eps**2:  # условие остановки\n",
    "            break\n",
    "        x_pred = x\n",
    "        print('Step: ', _)  # смотрим, на каком мы шаге\n",
    "        print('x_pred= ', x_pred)\n",
    "        print('f(x)', f(x))\n",
    "#         x = x-alpha*(20*x_pred)\n",
    "        x = x_pred - 20 * alpha * x_pred # по формуле выше\n",
    "        print('Gradient= ', x)\n",
    "        print('\\n')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 640,
     "status": "ok",
     "timestamp": 1538843113019,
     "user": {
      "displayName": "Konstantin Baltsat",
      "photoUrl": "",
      "userId": "06917529083593270168"
     },
     "user_tz": -180
    },
    "id": "9k8A7ei8pP9g",
    "outputId": "7b378852-708a-4baf-fc26-adf90ab38b19"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491b529d1a6c44428a99b170a619ed8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='1st loop', max=100000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0\n",
      "x_pred=  50\n",
      "f(x) 25000\n",
      "Gradient=  40.0\n",
      "\n",
      "\n",
      "Step:  1\n",
      "x_pred=  40.0\n",
      "f(x) 16000.0\n",
      "Gradient=  32.0\n",
      "\n",
      "\n",
      "Step:  2\n",
      "x_pred=  32.0\n",
      "f(x) 10240.0\n",
      "Gradient=  25.6\n",
      "\n",
      "\n",
      "Step:  3\n",
      "x_pred=  25.6\n",
      "f(x) 6553.600000000001\n",
      "Gradient=  20.48\n",
      "\n",
      "\n",
      "Step:  4\n",
      "x_pred=  20.48\n",
      "f(x) 4194.304\n",
      "Gradient=  16.384\n",
      "\n",
      "\n",
      "Step:  5\n",
      "x_pred=  16.384\n",
      "f(x) 2684.3545599999998\n",
      "Gradient=  13.1072\n",
      "\n",
      "\n",
      "Step:  6\n",
      "x_pred=  13.1072\n",
      "f(x) 1717.9869184\n",
      "Gradient=  10.48576\n",
      "\n",
      "\n",
      "Step:  7\n",
      "x_pred=  10.48576\n",
      "f(x) 1099.5116277760003\n",
      "Gradient=  8.388608000000001\n",
      "\n",
      "\n",
      "Step:  8\n",
      "x_pred=  8.388608000000001\n",
      "f(x) 703.6874417766403\n",
      "Gradient=  6.7108864000000015\n",
      "\n",
      "\n",
      "Step:  9\n",
      "x_pred=  6.7108864000000015\n",
      "f(x) 450.35996273704984\n",
      "Gradient=  5.368709120000001\n",
      "\n",
      "\n",
      "Step:  10\n",
      "x_pred=  5.368709120000001\n",
      "f(x) 288.2303761517118\n",
      "Gradient=  4.294967296000001\n",
      "\n",
      "\n",
      "Step:  11\n",
      "x_pred=  4.294967296000001\n",
      "f(x) 184.46744073709561\n",
      "Gradient=  3.435973836800001\n",
      "\n",
      "\n",
      "Step:  12\n",
      "x_pred=  3.435973836800001\n",
      "f(x) 118.0591620717412\n",
      "Gradient=  2.7487790694400007\n",
      "\n",
      "\n",
      "Step:  13\n",
      "x_pred=  2.7487790694400007\n",
      "f(x) 75.55786372591436\n",
      "Gradient=  2.1990232555520004\n",
      "\n",
      "\n",
      "Step:  14\n",
      "x_pred=  2.1990232555520004\n",
      "f(x) 48.35703278458518\n",
      "Gradient=  1.7592186044416003\n",
      "\n",
      "\n",
      "Step:  15\n",
      "x_pred=  1.7592186044416003\n",
      "f(x) 30.94850098213452\n",
      "Gradient=  1.4073748835532802\n",
      "\n",
      "\n",
      "Step:  16\n",
      "x_pred=  1.4073748835532802\n",
      "f(x) 19.807040628566092\n",
      "Gradient=  1.125899906842624\n",
      "\n",
      "\n",
      "Step:  17\n",
      "x_pred=  1.125899906842624\n",
      "f(x) 12.676506002282295\n",
      "Gradient=  0.9007199254740993\n",
      "\n",
      "\n",
      "Step:  18\n",
      "x_pred=  0.9007199254740993\n",
      "f(x) 8.112963841460669\n",
      "Gradient=  0.7205759403792794\n",
      "\n",
      "\n",
      "Step:  19\n",
      "x_pred=  0.7205759403792794\n",
      "f(x) 5.192296858534828\n",
      "Gradient=  0.5764607523034235\n",
      "\n",
      "\n",
      "Step:  20\n",
      "x_pred=  0.5764607523034235\n",
      "f(x) 3.32306998946229\n",
      "Gradient=  0.46116860184273883\n",
      "\n",
      "\n",
      "Step:  21\n",
      "x_pred=  0.46116860184273883\n",
      "f(x) 2.126764793255866\n",
      "Gradient=  0.36893488147419107\n",
      "\n",
      "\n",
      "Step:  22\n",
      "x_pred=  0.36893488147419107\n",
      "f(x) 1.361129467683754\n",
      "Gradient=  0.29514790517935285\n",
      "\n",
      "\n",
      "Step:  23\n",
      "x_pred=  0.29514790517935285\n",
      "f(x) 0.8711228593176026\n",
      "Gradient=  0.23611832414348227\n",
      "\n",
      "\n",
      "Step:  24\n",
      "x_pred=  0.23611832414348227\n",
      "f(x) 0.5575186299632656\n",
      "Gradient=  0.18889465931478583\n",
      "\n",
      "\n",
      "Step:  25\n",
      "x_pred=  0.18889465931478583\n",
      "f(x) 0.35681192317649\n",
      "Gradient=  0.15111572745182866\n",
      "\n",
      "\n",
      "Step:  26\n",
      "x_pred=  0.15111572745182866\n",
      "f(x) 0.22835963083295363\n",
      "Gradient=  0.12089258196146294\n",
      "\n",
      "\n",
      "Step:  27\n",
      "x_pred=  0.12089258196146294\n",
      "f(x) 0.14615016373309034\n",
      "Gradient=  0.09671406556917035\n",
      "\n",
      "\n",
      "Step:  28\n",
      "x_pred=  0.09671406556917035\n",
      "f(x) 0.09353610478917782\n",
      "Gradient=  0.07737125245533628\n",
      "\n",
      "\n",
      "Step:  29\n",
      "x_pred=  0.07737125245533628\n",
      "f(x) 0.05986310706507381\n",
      "Gradient=  0.061897001964269026\n",
      "\n",
      "\n",
      "Step:  30\n",
      "x_pred=  0.061897001964269026\n",
      "f(x) 0.038312388521647235\n",
      "Gradient=  0.04951760157141522\n",
      "\n",
      "\n",
      "Step:  31\n",
      "x_pred=  0.04951760157141522\n",
      "f(x) 0.02451992865385423\n",
      "Gradient=  0.03961408125713218\n",
      "\n",
      "\n",
      "Step:  32\n",
      "x_pred=  0.03961408125713218\n",
      "f(x) 0.01569275433846671\n",
      "Gradient=  0.031691265005705745\n",
      "\n",
      "\n",
      "Step:  33\n",
      "x_pred=  0.031691265005705745\n",
      "f(x) 0.010043362776618697\n",
      "Gradient=  0.025353012004564596\n",
      "\n",
      "\n",
      "Step:  34\n",
      "x_pred=  0.025353012004564596\n",
      "f(x) 0.006427752177035965\n",
      "Gradient=  0.02028240960365168\n",
      "\n",
      "\n",
      "Step:  35\n",
      "x_pred=  0.02028240960365168\n",
      "f(x) 0.004113761393303018\n",
      "Gradient=  0.016225927682921342\n",
      "\n",
      "\n",
      "Step:  36\n",
      "x_pred=  0.016225927682921342\n",
      "f(x) 0.0026328072917139317\n",
      "Gradient=  0.012980742146337074\n",
      "\n",
      "\n",
      "Step:  37\n",
      "x_pred=  0.012980742146337074\n",
      "f(x) 0.0016849966666969162\n",
      "Gradient=  0.01038459371706966\n",
      "\n",
      "\n",
      "Step:  38\n",
      "x_pred=  0.01038459371706966\n",
      "f(x) 0.0010783978666860266\n",
      "Gradient=  0.008307674973655728\n",
      "\n",
      "\n",
      "Step:  39\n",
      "x_pred=  0.008307674973655728\n",
      "f(x) 0.000690174634679057\n",
      "Gradient=  0.006646139978924583\n",
      "\n",
      "\n",
      "Step:  40\n",
      "x_pred=  0.006646139978924583\n",
      "f(x) 0.0004417117661945965\n",
      "Gradient=  0.005316911983139666\n",
      "\n",
      "\n",
      "Step:  41\n",
      "x_pred=  0.005316911983139666\n",
      "f(x) 0.00028269553036454174\n",
      "Gradient=  0.004253529586511732\n",
      "\n",
      "\n",
      "Step:  42\n",
      "x_pred=  0.004253529586511732\n",
      "f(x) 0.0001809251394333067\n",
      "Gradient=  0.003402823669209386\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_min = gradient_descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 636,
     "status": "ok",
     "timestamp": 1538842916473,
     "user": {
      "displayName": "Konstantin Baltsat",
      "photoUrl": "",
      "userId": "06917529083593270168"
     },
     "user_tz": -180
    },
    "id": "uGOVAybRpP9k",
    "outputId": "3a1e65cc-a195-4b67-fa95-17f80f83cd9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.012380495"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 581,
     "status": "ok",
     "timestamp": 1538842938177,
     "user": {
      "displayName": "Konstantin Baltsat",
      "photoUrl": "",
      "userId": "06917529083593270168"
     },
     "user_tz": -180
    },
    "id": "_EW8AY8VpP9n",
    "outputId": "f7e5c27b-1d62-4c2c-d381-a4dfe394f25c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00153276656445025"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tsqfsTezpP9q"
   },
   "source": [
    "* *Пример 2*: Посчитаем формулу градиентного спуска для функции $f(x, y) = 10x^2 + y^2$:   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MNWMBcbNpP9r"
   },
   "source": [
    "$$\\left(\\begin{matrix} x^i \\\\ y^i \\end{matrix}\\right) = \\left(\\begin{matrix} x^{i-1} \\\\ y^{i-1} \\end{matrix}\\right) - \\alpha \\nabla f(x^{i-1}, y^{i-1}) = \\left(\\begin{matrix} x^{i-1} \\\\ y^{i-1} \\end{matrix}\\right) - \\alpha \\left(\\begin{matrix} \\frac{\\partial{f(x^{i-1}, y^{i-1})}}{\\partial{x}} \\\\ \\frac{\\partial{f(x^{i-1}, y^{i-1})}}{\\partial{y}} \\end{matrix}\\right) = x^{i-1} - \\alpha \\left(\\begin{matrix} 20x^{i-1} \\\\ 2y^{i-1} \\end{matrix}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sBnijsKLpP9r"
   },
   "source": [
    "Осталось написать код, выполняющий градиентный спуск, пока не выполнится условие остановки, для функции $f(x, y) = 10x^2 + y^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p_rDsja-pP9s"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def f(x):\n",
    "    return 10 * x[0]**2 + x[1]**2\n",
    "\n",
    "def gradient_descent(alpha=0.01, eps=0.001):\n",
    "    x_prev = np.array([100, 100])  # начальная инициализация\n",
    "    x = np.array([50, 50])  # начальная инициализация\n",
    "    for _ in tqdm_notebook(range(100000)):\n",
    "        if np.sum((x - x_prev)**2) < eps**2:  # условие остановки\n",
    "            break\n",
    "        x_prev = x\n",
    "        print('Step: {}\\nx_prev: {}\\nf(x): {}'.format(_, x_prev, f(x)))  # смотрим, на каком мы шаге\n",
    "        x = x_prev - alpha * np.array(20 * x_prev[0], 2 * x_prev[1])  # по формуле выше\n",
    "        print('Gradient:{}\\n'.format(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 848
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 599,
     "status": "ok",
     "timestamp": 1538833258362,
     "user": {
      "displayName": "Григорий Алексеевич Лелейтнер",
      "photoUrl": "",
      "userId": "07080665896519552124"
     },
     "user_tz": -300
    },
    "id": "boueQCnXpP9u",
    "outputId": "4f322bce-2c6b-4048-c471-5a18b49dc40b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb7849e479e04ea59b5009adf8bdae27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n",
      "x_prev: [50 50]\n",
      "f(x): 27500\n",
      "Gradient:[40. 40.]\n",
      "\n",
      "Step: 1\n",
      "x_prev: [40. 40.]\n",
      "f(x): 17600.0\n",
      "Gradient:[32. 32.]\n",
      "\n",
      "Step: 2\n",
      "x_prev: [32. 32.]\n",
      "f(x): 11264.0\n",
      "Gradient:[25.6 25.6]\n",
      "\n",
      "Step: 3\n",
      "x_prev: [25.6 25.6]\n",
      "f(x): 7208.960000000001\n",
      "Gradient:[20.48 20.48]\n",
      "\n",
      "Step: 4\n",
      "x_prev: [20.48 20.48]\n",
      "f(x): 4613.7344\n",
      "Gradient:[16.384 16.384]\n",
      "\n",
      "Step: 5\n",
      "x_prev: [16.384 16.384]\n",
      "f(x): 2952.790016\n",
      "Gradient:[13.1072 13.1072]\n",
      "\n",
      "Step: 6\n",
      "x_prev: [13.1072 13.1072]\n",
      "f(x): 1889.7856102399999\n",
      "Gradient:[10.48576 10.48576]\n",
      "\n",
      "Step: 7\n",
      "x_prev: [10.48576 10.48576]\n",
      "f(x): 1209.4627905536004\n",
      "Gradient:[8.388608 8.388608]\n",
      "\n",
      "Step: 8\n",
      "x_prev: [8.388608 8.388608]\n",
      "f(x): 774.0561859543043\n",
      "Gradient:[6.7108864 6.7108864]\n",
      "\n",
      "Step: 9\n",
      "x_prev: [6.7108864 6.7108864]\n",
      "f(x): 495.39595901075484\n",
      "Gradient:[5.36870912 5.36870912]\n",
      "\n",
      "Step: 10\n",
      "x_prev: [5.36870912 5.36870912]\n",
      "f(x): 317.053413766883\n",
      "Gradient:[4.2949673 4.2949673]\n",
      "\n",
      "Step: 11\n",
      "x_prev: [4.2949673 4.2949673]\n",
      "f(x): 202.91418481080518\n",
      "Gradient:[3.43597384 3.43597384]\n",
      "\n",
      "Step: 12\n",
      "x_prev: [3.43597384 3.43597384]\n",
      "f(x): 129.8650782789153\n",
      "Gradient:[2.74877907 2.74877907]\n",
      "\n",
      "Step: 13\n",
      "x_prev: [2.74877907 2.74877907]\n",
      "f(x): 83.11365009850576\n",
      "Gradient:[2.19902326 2.19902326]\n",
      "\n",
      "Step: 14\n",
      "x_prev: [2.19902326 2.19902326]\n",
      "f(x): 53.1927360630437\n",
      "Gradient:[1.7592186 1.7592186]\n",
      "\n",
      "Step: 15\n",
      "x_prev: [1.7592186 1.7592186]\n",
      "f(x): 34.04335108034797\n",
      "Gradient:[1.40737488 1.40737488]\n",
      "\n",
      "Step: 16\n",
      "x_prev: [1.40737488 1.40737488]\n",
      "f(x): 21.787744691422702\n",
      "Gradient:[1.12589991 1.12589991]\n",
      "\n",
      "Step: 17\n",
      "x_prev: [1.12589991 1.12589991]\n",
      "f(x): 13.944156602510525\n",
      "Gradient:[0.90071993 0.90071993]\n",
      "\n",
      "Step: 18\n",
      "x_prev: [0.90071993 0.90071993]\n",
      "f(x): 8.924260225606734\n",
      "Gradient:[0.72057594 0.72057594]\n",
      "\n",
      "Step: 19\n",
      "x_prev: [0.72057594 0.72057594]\n",
      "f(x): 5.71152654438831\n",
      "Gradient:[0.57646075 0.57646075]\n",
      "\n",
      "Step: 20\n",
      "x_prev: [0.57646075 0.57646075]\n",
      "f(x): 3.6553769884085177\n",
      "Gradient:[0.4611686 0.4611686]\n",
      "\n",
      "Step: 21\n",
      "x_prev: [0.4611686 0.4611686]\n",
      "f(x): 2.339441272581451\n",
      "Gradient:[0.36893488 0.36893488]\n",
      "\n",
      "Step: 22\n",
      "x_prev: [0.36893488 0.36893488]\n",
      "f(x): 1.4972424144521284\n",
      "Gradient:[0.29514791 0.29514791]\n",
      "\n",
      "Step: 23\n",
      "x_prev: [0.29514791 0.29514791]\n",
      "f(x): 0.9582351452493622\n",
      "Gradient:[0.23611832 0.23611832]\n",
      "\n",
      "Step: 24\n",
      "x_prev: [0.23611832 0.23611832]\n",
      "f(x): 0.6132704929595917\n",
      "Gradient:[0.18889466 0.18889466]\n",
      "\n",
      "Step: 25\n",
      "x_prev: [0.18889466 0.18889466]\n",
      "f(x): 0.3924931154941387\n",
      "Gradient:[0.15111573 0.15111573]\n",
      "\n",
      "Step: 26\n",
      "x_prev: [0.15111573 0.15111573]\n",
      "f(x): 0.2511955939162488\n",
      "Gradient:[0.12089258 0.12089258]\n",
      "\n",
      "Step: 27\n",
      "x_prev: [0.12089258 0.12089258]\n",
      "f(x): 0.16076518010639923\n",
      "Gradient:[0.09671407 0.09671407]\n",
      "\n",
      "Step: 28\n",
      "x_prev: [0.09671407 0.09671407]\n",
      "f(x): 0.10288971526809551\n",
      "Gradient:[0.07737125 0.07737125]\n",
      "\n",
      "Step: 29\n",
      "x_prev: [0.07737125 0.07737125]\n",
      "f(x): 0.06584941777158111\n",
      "Gradient:[0.061897 0.061897]\n",
      "\n",
      "Step: 30\n",
      "x_prev: [0.061897 0.061897]\n",
      "f(x): 0.042143627373811915\n",
      "Gradient:[0.0495176 0.0495176]\n",
      "\n",
      "Step: 31\n",
      "x_prev: [0.0495176 0.0495176]\n",
      "f(x): 0.026971921519239623\n",
      "Gradient:[0.03961408 0.03961408]\n",
      "\n",
      "Step: 32\n",
      "x_prev: [0.03961408 0.03961408]\n",
      "f(x): 0.01726202977231336\n",
      "Gradient:[0.03169127 0.03169127]\n",
      "\n",
      "Step: 33\n",
      "x_prev: [0.03169127 0.03169127]\n",
      "f(x): 0.011047699054280552\n",
      "Gradient:[0.02535301 0.02535301]\n",
      "\n",
      "Step: 34\n",
      "x_prev: [0.02535301 0.02535301]\n",
      "f(x): 0.007070527394739552\n",
      "Gradient:[0.02028241 0.02028241]\n",
      "\n",
      "Step: 35\n",
      "x_prev: [0.02028241 0.02028241]\n",
      "f(x): 0.0045251375326333144\n",
      "Gradient:[0.01622593 0.01622593]\n",
      "\n",
      "Step: 36\n",
      "x_prev: [0.01622593 0.01622593]\n",
      "f(x): 0.002896088020885321\n",
      "Gradient:[0.01298074 0.01298074]\n",
      "\n",
      "Step: 37\n",
      "x_prev: [0.01298074 0.01298074]\n",
      "f(x): 0.0018534963333666054\n",
      "Gradient:[0.01038459 0.01038459]\n",
      "\n",
      "Step: 38\n",
      "x_prev: [0.01038459 0.01038459]\n",
      "f(x): 0.0011862376533546275\n",
      "Gradient:[0.00830767 0.00830767]\n",
      "\n",
      "Step: 39\n",
      "x_prev: [0.00830767 0.00830767]\n",
      "f(x): 0.0007591920981469619\n",
      "Gradient:[0.00664614 0.00664614]\n",
      "\n",
      "Step: 40\n",
      "x_prev: [0.00664614 0.00664614]\n",
      "f(x): 0.0004858829428140555\n",
      "Gradient:[0.00531691 0.00531691]\n",
      "\n",
      "Step: 41\n",
      "x_prev: [0.00531691 0.00531691]\n",
      "f(x): 0.0003109650834009955\n",
      "Gradient:[0.00425353 0.00425353]\n",
      "\n",
      "Step: 42\n",
      "x_prev: [0.00425353 0.00425353]\n",
      "f(x): 0.0001990176533766371\n",
      "Gradient:[0.00340282 0.00340282]\n",
      "\n",
      "Step: 43\n",
      "x_prev: [0.00340282 0.00340282]\n",
      "f(x): 0.00012737129816104776\n",
      "Gradient:[0.00272226 0.00272226]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_min = gradient_descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 869,
     "status": "ok",
     "timestamp": 1538833263482,
     "user": {
      "displayName": "Григорий Алексеевич Лелейтнер",
      "photoUrl": "",
      "userId": "07080665896519552124"
     },
     "user_tz": -300
    },
    "id": "6pyhQsmXpP9x",
    "outputId": "7fbba388-a1b1-4e81-9996-e4e88305bf84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23611832, 0.23611832])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 760,
     "status": "ok",
     "timestamp": 1538833264359,
     "user": {
      "displayName": "Григорий Алексеевич Лелейтнер",
      "photoUrl": "",
      "userId": "07080665896519552124"
     },
     "user_tz": -300
    },
    "id": "ytAfn_X7pP90",
    "outputId": "90d9a8f8-24a7-4741-ba91-af01770726ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6132704929595917"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 1/(1+np.e**(-x[0]*x[1]))\n",
    "\n",
    "def gradient_descent(alpha=0.01, eps=0.001):\n",
    "    x_prev = np.array([0, 0]) # начальная инициализация\n",
    "    x = np.array([1,1])  # начальная инициализация\n",
    "    for _ in tqdm_notebook(range(200)):\n",
    "        print('x-x_prev:{} < eps: {}\\n'.format(np.sum((x - x_prev)**2), eps**2))\n",
    "        if np.sum((x - x_prev)**2) < eps**2:  # условие остановки\n",
    "            break\n",
    "        x_prev = x\n",
    "        print('Step: {}\\nx_prev: {}\\nf(x): {}'.format(_, x_prev, f(x)))  # смотрим, на каком мы шаге\n",
    "        x = x_prev - alpha * np.array(np.e**(x_prev[0]*x_prev[1])/(np.e**(x_prev[0]*x_prev[1])+1)**2,\n",
    "                                      np.e**(x_prev[0]*x_prev[1])/(np.e**(x_prev[0]*x_prev[1])+1)**2 )  # по формуле выше\n",
    "        print('Gradient:{}\\n'.format(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7adc06fe39d84216b2d8bf4e336f65c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x-x_prev:2 < eps: 1e-06\n",
      "\n",
      "Step: 0\n",
      "x_prev: [1 1]\n",
      "f(x): 0.7310585786300049\n",
      "Gradient:[0.99803388 0.99803388]\n",
      "\n",
      "x-x_prev:7.731250458590924e-06 < eps: 1e-06\n",
      "\n",
      "Step: 1\n",
      "x_prev: [0.99803388 0.99803388]\n",
      "f(x): 0.7302855129067523\n",
      "Gradient:[0.99606419 0.99606419]\n",
      "\n",
      "x-x_prev:7.759324501979046e-06 < eps: 1e-06\n",
      "\n",
      "Step: 2\n",
      "x_prev: [0.99606419 0.99606419]\n",
      "f(x): 0.7295111708045297\n",
      "Gradient:[0.99409095 0.99409095]\n",
      "\n",
      "x-x_prev:7.787401327054148e-06 < eps: 1e-06\n",
      "\n",
      "Step: 3\n",
      "x_prev: [0.99409095 0.99409095]\n",
      "f(x): 0.7287355659040435\n",
      "Gradient:[0.99211415 0.99211415]\n",
      "\n",
      "x-x_prev:7.815479713298511e-06 < eps: 1e-06\n",
      "\n",
      "Step: 4\n",
      "x_prev: [0.99211415 0.99211415]\n",
      "f(x): 0.7279587119340227\n",
      "Gradient:[0.9901338 0.9901338]\n",
      "\n",
      "x-x_prev:7.843558434313041e-06 < eps: 1e-06\n",
      "\n",
      "Step: 5\n",
      "x_prev: [0.9901338 0.9901338]\n",
      "f(x): 0.7271806227706714\n",
      "Gradient:[0.98814991 0.98814991]\n",
      "\n",
      "x-x_prev:7.87163625799025e-06 < eps: 1e-06\n",
      "\n",
      "Step: 6\n",
      "x_prev: [0.98814991 0.98814991]\n",
      "f(x): 0.7264013124370923\n",
      "Gradient:[0.98616249 0.98616249]\n",
      "\n",
      "x-x_prev:7.899711946691053e-06 < eps: 1e-06\n",
      "\n",
      "Step: 7\n",
      "x_prev: [0.98616249 0.98616249]\n",
      "f(x): 0.7256207951026812\n",
      "Gradient:[0.98417153 0.98417153]\n",
      "\n",
      "x-x_prev:7.927784257424456e-06 < eps: 1e-06\n",
      "\n",
      "Step: 8\n",
      "x_prev: [0.98417153 0.98417153]\n",
      "f(x): 0.7248390850824916\n",
      "Gradient:[0.98217706 0.98217706]\n",
      "\n",
      "x-x_prev:7.955851942028433e-06 < eps: 1e-06\n",
      "\n",
      "Step: 9\n",
      "x_prev: [0.98217706 0.98217706]\n",
      "f(x): 0.7240561968365714\n",
      "Gradient:[0.98017907 0.98017907]\n",
      "\n",
      "x-x_prev:7.983913747356328e-06 < eps: 1e-06\n",
      "\n",
      "Step: 10\n",
      "x_prev: [0.98017907 0.98017907]\n",
      "f(x): 0.723272144969269\n",
      "Gradient:[0.97817758 0.97817758]\n",
      "\n",
      "x-x_prev:8.011968415463546e-06 < eps: 1e-06\n",
      "\n",
      "Step: 11\n",
      "x_prev: [0.97817758 0.97817758]\n",
      "f(x): 0.7224869442285106\n",
      "Gradient:[0.97617258 0.97617258]\n",
      "\n",
      "x-x_prev:8.04001468379714e-06 < eps: 1e-06\n",
      "\n",
      "Step: 12\n",
      "x_prev: [0.97617258 0.97617258]\n",
      "f(x): 0.7217006095050486\n",
      "Gradient:[0.97416409 0.97416409]\n",
      "\n",
      "x-x_prev:8.068051285390058e-06 < eps: 1e-06\n",
      "\n",
      "Step: 13\n",
      "x_prev: [0.97416409 0.97416409]\n",
      "f(x): 0.7209131558316803\n",
      "Gradient:[0.97215212 0.97215212]\n",
      "\n",
      "x-x_prev:8.096076949052971e-06 < eps: 1e-06\n",
      "\n",
      "Step: 14\n",
      "x_prev: [0.97215212 0.97215212]\n",
      "f(x): 0.7201245983824375\n",
      "Gradient:[0.97013667 0.97013667]\n",
      "\n",
      "x-x_prev:8.124090399576082e-06 < eps: 1e-06\n",
      "\n",
      "Step: 15\n",
      "x_prev: [0.97013667 0.97013667]\n",
      "f(x): 0.7193349524717466\n",
      "Gradient:[0.96811774 0.96811774]\n",
      "\n",
      "x-x_prev:8.152090357926705e-06 < eps: 1e-06\n",
      "\n",
      "Step: 16\n",
      "x_prev: [0.96811774 0.96811774]\n",
      "f(x): 0.7185442335535597\n",
      "Gradient:[0.96609536 0.96609536]\n",
      "\n",
      "x-x_prev:8.180075541449677e-06 < eps: 1e-06\n",
      "\n",
      "Step: 17\n",
      "x_prev: [0.96609536 0.96609536]\n",
      "f(x): 0.7177524572204557\n",
      "Gradient:[0.96406952 0.96406952]\n",
      "\n",
      "x-x_prev:8.208044664077804e-06 < eps: 1e-06\n",
      "\n",
      "Step: 18\n",
      "x_prev: [0.96406952 0.96406952]\n",
      "f(x): 0.7169596392027141\n",
      "Gradient:[0.96204024 0.96204024]\n",
      "\n",
      "x-x_prev:8.235996436532598e-06 < eps: 1e-06\n",
      "\n",
      "Step: 19\n",
      "x_prev: [0.96204024 0.96204024]\n",
      "f(x): 0.7161657953673569\n",
      "Gradient:[0.96000751 0.96000751]\n",
      "\n",
      "x-x_prev:8.26392956653771e-06 < eps: 1e-06\n",
      "\n",
      "Step: 20\n",
      "x_prev: [0.96000751 0.96000751]\n",
      "f(x): 0.7153709417171644\n",
      "Gradient:[0.95797136 0.95797136]\n",
      "\n",
      "x-x_prev:8.291842759027094e-06 < eps: 1e-06\n",
      "\n",
      "Step: 21\n",
      "x_prev: [0.95797136 0.95797136]\n",
      "f(x): 0.7145750943896597\n",
      "Gradient:[0.95593178 0.95593178]\n",
      "\n",
      "x-x_prev:8.319734716361365e-06 < eps: 1e-06\n",
      "\n",
      "Step: 22\n",
      "x_prev: [0.95593178 0.95593178]\n",
      "f(x): 0.7137782696560657\n",
      "Gradient:[0.9538888 0.9538888]\n",
      "\n",
      "x-x_prev:8.347604138539735e-06 < eps: 1e-06\n",
      "\n",
      "Step: 23\n",
      "x_prev: [0.9538888 0.9538888]\n",
      "f(x): 0.7129804839202324\n",
      "Gradient:[0.9518424 0.9518424]\n",
      "\n",
      "x-x_prev:8.375449723421044e-06 < eps: 1e-06\n",
      "\n",
      "Step: 24\n",
      "x_prev: [0.9518424 0.9518424]\n",
      "f(x): 0.7121817537175368\n",
      "Gradient:[0.94979261 0.94979261]\n",
      "\n",
      "x-x_prev:8.403270166940286e-06 < eps: 1e-06\n",
      "\n",
      "Step: 25\n",
      "x_prev: [0.94979261 0.94979261]\n",
      "f(x): 0.7113820957137524\n",
      "Gradient:[0.94773944 0.94773944]\n",
      "\n",
      "x-x_prev:8.431064163329697e-06 < eps: 1e-06\n",
      "\n",
      "Step: 26\n",
      "x_prev: [0.94773944 0.94773944]\n",
      "f(x): 0.7105815267038926\n",
      "Gradient:[0.94568288 0.94568288]\n",
      "\n",
      "x-x_prev:8.458830405342538e-06 < eps: 1e-06\n",
      "\n",
      "Step: 27\n",
      "x_prev: [0.94568288 0.94568288]\n",
      "f(x): 0.7097800636110239\n",
      "Gradient:[0.94362296 0.94362296]\n",
      "\n",
      "x-x_prev:8.486567584476856e-06 < eps: 1e-06\n",
      "\n",
      "Step: 28\n",
      "x_prev: [0.94362296 0.94362296]\n",
      "f(x): 0.7089777234850517\n",
      "Gradient:[0.94155968 0.94155968]\n",
      "\n",
      "x-x_prev:8.514274391202825e-06 < eps: 1e-06\n",
      "\n",
      "Step: 29\n",
      "x_prev: [0.94155968 0.94155968]\n",
      "f(x): 0.7081745235014787\n",
      "Gradient:[0.93949304 0.93949304]\n",
      "\n",
      "x-x_prev:8.541949515189082e-06 < eps: 1e-06\n",
      "\n",
      "Step: 30\n",
      "x_prev: [0.93949304 0.93949304]\n",
      "f(x): 0.707370480960135\n",
      "Gradient:[0.93742307 0.93742307]\n",
      "\n",
      "x-x_prev:8.56959164552894e-06 < eps: 1e-06\n",
      "\n",
      "Step: 31\n",
      "x_prev: [0.93742307 0.93742307]\n",
      "f(x): 0.7065656132838806\n",
      "Gradient:[0.93534976 0.93534976]\n",
      "\n",
      "x-x_prev:8.59719947097836e-06 < eps: 1e-06\n",
      "\n",
      "Step: 32\n",
      "x_prev: [0.93534976 0.93534976]\n",
      "f(x): 0.7057599380172811\n",
      "Gradient:[0.93327313 0.93327313]\n",
      "\n",
      "x-x_prev:8.624771680179101e-06 < eps: 1e-06\n",
      "\n",
      "Step: 33\n",
      "x_prev: [0.93327313 0.93327313]\n",
      "f(x): 0.7049534728252559\n",
      "Gradient:[0.93119319 0.93119319]\n",
      "\n",
      "x-x_prev:8.652306961899077e-06 < eps: 1e-06\n",
      "\n",
      "Step: 34\n",
      "x_prev: [0.93119319 0.93119319]\n",
      "f(x): 0.7041462354916993\n",
      "Gradient:[0.92910995 0.92910995]\n",
      "\n",
      "x-x_prev:8.679804005259626e-06 < eps: 1e-06\n",
      "\n",
      "Step: 35\n",
      "x_prev: [0.92910995 0.92910995]\n",
      "f(x): 0.703338243918076\n",
      "Gradient:[0.92702341 0.92702341]\n",
      "\n",
      "x-x_prev:8.707261499976327e-06 < eps: 1e-06\n",
      "\n",
      "Step: 36\n",
      "x_prev: [0.92702341 0.92702341]\n",
      "f(x): 0.7025295161219887\n",
      "Gradient:[0.9249336 0.9249336]\n",
      "\n",
      "x-x_prev:8.734678136593054e-06 < eps: 1e-06\n",
      "\n",
      "Step: 37\n",
      "x_prev: [0.9249336 0.9249336]\n",
      "f(x): 0.7017200702357211\n",
      "Gradient:[0.92284051 0.92284051]\n",
      "\n",
      "x-x_prev:8.762052606717551e-06 < eps: 1e-06\n",
      "\n",
      "Step: 38\n",
      "x_prev: [0.92284051 0.92284051]\n",
      "f(x): 0.7009099245047542\n",
      "Gradient:[0.92074415 0.92074415]\n",
      "\n",
      "x-x_prev:8.789383603263119e-06 < eps: 1e-06\n",
      "\n",
      "Step: 39\n",
      "x_prev: [0.92074415 0.92074415]\n",
      "f(x): 0.7000990972862567\n",
      "Gradient:[0.91864455 0.91864455]\n",
      "\n",
      "x-x_prev:8.816669820683412e-06 < eps: 1e-06\n",
      "\n",
      "Step: 40\n",
      "x_prev: [0.91864455 0.91864455]\n",
      "f(x): 0.6992876070475512\n",
      "Gradient:[0.91654171 0.91654171]\n",
      "\n",
      "x-x_prev:8.843909955213283e-06 < eps: 1e-06\n",
      "\n",
      "Step: 41\n",
      "x_prev: [0.91654171 0.91654171]\n",
      "f(x): 0.6984754723645552\n",
      "Gradient:[0.91443563 0.91443563]\n",
      "\n",
      "x-x_prev:8.871102705111038e-06 < eps: 1e-06\n",
      "\n",
      "Step: 42\n",
      "x_prev: [0.91443563 0.91443563]\n",
      "f(x): 0.6976627119201967\n",
      "Gradient:[0.91232634 0.91232634]\n",
      "\n",
      "x-x_prev:8.89824677089551e-06 < eps: 1e-06\n",
      "\n",
      "Step: 43\n",
      "x_prev: [0.91232634 0.91232634]\n",
      "f(x): 0.6968493445028059\n",
      "Gradient:[0.91021383 0.91021383]\n",
      "\n",
      "x-x_prev:8.925340855587248e-06 < eps: 1e-06\n",
      "\n",
      "Step: 44\n",
      "x_prev: [0.91021383 0.91021383]\n",
      "f(x): 0.6960353890044838\n",
      "Gradient:[0.90809813 0.90809813]\n",
      "\n",
      "x-x_prev:8.952383664953796e-06 < eps: 1e-06\n",
      "\n",
      "Step: 45\n",
      "x_prev: [0.90809813 0.90809813]\n",
      "f(x): 0.6952208644194456\n",
      "Gradient:[0.90597924 0.90597924]\n",
      "\n",
      "x-x_prev:8.97937390774498e-06 < eps: 1e-06\n",
      "\n",
      "Step: 46\n",
      "x_prev: [0.90597924 0.90597924]\n",
      "f(x): 0.694405789842343\n",
      "Gradient:[0.90385718 0.90385718]\n",
      "\n",
      "x-x_prev:9.006310295939727e-06 < eps: 1e-06\n",
      "\n",
      "Step: 47\n",
      "x_prev: [0.90385718 0.90385718]\n",
      "f(x): 0.6935901844665607\n",
      "Gradient:[0.90173195 0.90173195]\n",
      "\n",
      "x-x_prev:9.033191544984661e-06 < eps: 1e-06\n",
      "\n",
      "Step: 48\n",
      "x_prev: [0.90173195 0.90173195]\n",
      "f(x): 0.6927740675824938\n",
      "Gradient:[0.89960357 0.89960357]\n",
      "\n",
      "x-x_prev:9.060016374038533e-06 < eps: 1e-06\n",
      "\n",
      "Step: 49\n",
      "x_prev: [0.89960357 0.89960357]\n",
      "f(x): 0.6919574585758003\n",
      "Gradient:[0.89747205 0.89747205]\n",
      "\n",
      "x-x_prev:9.086783506210221e-06 < eps: 1e-06\n",
      "\n",
      "Step: 50\n",
      "x_prev: [0.89747205 0.89747205]\n",
      "f(x): 0.6911403769256336\n",
      "Gradient:[0.89533739 0.89533739]\n",
      "\n",
      "x-x_prev:9.113491668802486e-06 < eps: 1e-06\n",
      "\n",
      "Step: 51\n",
      "x_prev: [0.89533739 0.89533739]\n",
      "f(x): 0.6903228422028532\n",
      "Gradient:[0.89319962 0.89319962]\n",
      "\n",
      "x-x_prev:9.140139593552984e-06 < eps: 1e-06\n",
      "\n",
      "Step: 52\n",
      "x_prev: [0.89319962 0.89319962]\n",
      "f(x): 0.6895048740682143\n",
      "Gradient:[0.89105874 0.89105874]\n",
      "\n",
      "x-x_prev:9.166726016872457e-06 < eps: 1e-06\n",
      "\n",
      "Step: 53\n",
      "x_prev: [0.89105874 0.89105874]\n",
      "f(x): 0.6886864922705377\n",
      "Gradient:[0.88891477 0.88891477]\n",
      "\n",
      "x-x_prev:9.193249680090512e-06 < eps: 1e-06\n",
      "\n",
      "Step: 54\n",
      "x_prev: [0.88891477 0.88891477]\n",
      "f(x): 0.6878677166448587\n",
      "Gradient:[0.88676771 0.88676771]\n",
      "\n",
      "x-x_prev:9.219709329688996e-06 < eps: 1e-06\n",
      "\n",
      "Step: 55\n",
      "x_prev: [0.88676771 0.88676771]\n",
      "f(x): 0.6870485671105576\n",
      "Gradient:[0.88461758 0.88461758]\n",
      "\n",
      "x-x_prev:9.24610371754192e-06 < eps: 1e-06\n",
      "\n",
      "Step: 56\n",
      "x_prev: [0.88461758 0.88461758]\n",
      "f(x): 0.6862290636694706\n",
      "Gradient:[0.88246439 0.88246439]\n",
      "\n",
      "x-x_prev:9.272431601160026e-06 < eps: 1e-06\n",
      "\n",
      "Step: 57\n",
      "x_prev: [0.88246439 0.88246439]\n",
      "f(x): 0.6854092264039818\n",
      "Gradient:[0.88030816 0.88030816]\n",
      "\n",
      "x-x_prev:9.298691743918946e-06 < eps: 1e-06\n",
      "\n",
      "Step: 58\n",
      "x_prev: [0.88030816 0.88030816]\n",
      "f(x): 0.684589075475098\n",
      "Gradient:[0.87814889 0.87814889]\n",
      "\n",
      "x-x_prev:9.324882915302426e-06 < eps: 1e-06\n",
      "\n",
      "Step: 59\n",
      "x_prev: [0.87814889 0.87814889]\n",
      "f(x): 0.6837686311205046\n",
      "Gradient:[0.8759866 0.8759866]\n",
      "\n",
      "x-x_prev:9.351003891135707e-06 < eps: 1e-06\n",
      "\n",
      "Step: 60\n",
      "x_prev: [0.8759866 0.8759866]\n",
      "f(x): 0.6829479136526057\n",
      "Gradient:[0.8738213 0.8738213]\n",
      "\n",
      "x-x_prev:9.37705345381956e-06 < eps: 1e-06\n",
      "\n",
      "Step: 61\n",
      "x_prev: [0.8738213 0.8738213]\n",
      "f(x): 0.6821269434565469\n",
      "Gradient:[0.871653 0.871653]\n",
      "\n",
      "x-x_prev:9.403030392561068e-06 < eps: 1e-06\n",
      "\n",
      "Step: 62\n",
      "x_prev: [0.871653 0.871653]\n",
      "f(x): 0.6813057409882219\n",
      "Gradient:[0.86948172 0.86948172]\n",
      "\n",
      "x-x_prev:9.4289335036117e-06 < eps: 1e-06\n",
      "\n",
      "Step: 63\n",
      "x_prev: [0.86948172 0.86948172]\n",
      "f(x): 0.6804843267722638\n",
      "Gradient:[0.86730747 0.86730747]\n",
      "\n",
      "x-x_prev:9.454761590491473e-06 < eps: 1e-06\n",
      "\n",
      "Step: 64\n",
      "x_prev: [0.86730747 0.86730747]\n",
      "f(x): 0.6796627214000209\n",
      "Gradient:[0.86513025 0.86513025]\n",
      "\n",
      "x-x_prev:9.480513464222257e-06 < eps: 1e-06\n",
      "\n",
      "Step: 65\n",
      "x_prev: [0.86513025 0.86513025]\n",
      "f(x): 0.6788409455275187\n",
      "Gradient:[0.86295009 0.86295009]\n",
      "\n",
      "x-x_prev:9.506187943550863e-06 < eps: 1e-06\n",
      "\n",
      "Step: 66\n",
      "x_prev: [0.86295009 0.86295009]\n",
      "f(x): 0.6780190198734072\n",
      "Gradient:[0.860767 0.860767]\n",
      "\n",
      "x-x_prev:9.531783855181246e-06 < eps: 1e-06\n",
      "\n",
      "Step: 67\n",
      "x_prev: [0.860767 0.860767]\n",
      "f(x): 0.6771969652168949\n",
      "Gradient:[0.85858099 0.85858099]\n",
      "\n",
      "x-x_prev:9.557300033992526e-06 < eps: 1e-06\n",
      "\n",
      "Step: 68\n",
      "x_prev: [0.85858099 0.85858099]\n",
      "f(x): 0.6763748023956699\n",
      "Gradient:[0.85639207 0.85639207]\n",
      "\n",
      "x-x_prev:9.582735323263087e-06 < eps: 1e-06\n",
      "\n",
      "Step: 69\n",
      "x_prev: [0.85639207 0.85639207]\n",
      "f(x): 0.6755525523038097\n",
      "Gradient:[0.85420026 0.85420026]\n",
      "\n",
      "x-x_prev:9.608088574893016e-06 < eps: 1e-06\n",
      "\n",
      "Step: 70\n",
      "x_prev: [0.85420026 0.85420026]\n",
      "f(x): 0.6747302358896764\n",
      "Gradient:[0.85200556 0.85200556]\n",
      "\n",
      "x-x_prev:9.63335864961896e-06 < eps: 1e-06\n",
      "\n",
      "Step: 71\n",
      "x_prev: [0.85200556 0.85200556]\n",
      "f(x): 0.6739078741538045\n",
      "Gradient:[0.849808 0.849808]\n",
      "\n",
      "x-x_prev:9.658544417235961e-06 < eps: 1e-06\n",
      "\n",
      "Step: 72\n",
      "x_prev: [0.849808 0.849808]\n",
      "f(x): 0.6730854881467743\n",
      "Gradient:[0.84760759 0.84760759]\n",
      "\n",
      "x-x_prev:9.683644756803827e-06 < eps: 1e-06\n",
      "\n",
      "Step: 73\n",
      "x_prev: [0.84760759 0.84760759]\n",
      "f(x): 0.6722630989670783\n",
      "Gradient:[0.84540433 0.84540433]\n",
      "\n",
      "x-x_prev:9.708658556866227e-06 < eps: 1e-06\n",
      "\n",
      "Step: 74\n",
      "x_prev: [0.84540433 0.84540433]\n",
      "f(x): 0.6714407277589758\n",
      "Gradient:[0.84319825 0.84319825]\n",
      "\n",
      "x-x_prev:9.733584715656176e-06 < eps: 1e-06\n",
      "\n",
      "Step: 75\n",
      "x_prev: [0.84319825 0.84319825]\n",
      "f(x): 0.6706183957103407\n",
      "Gradient:[0.84098936 0.84098936]\n",
      "\n",
      "x-x_prev:9.758422141305406e-06 < eps: 1e-06\n",
      "\n",
      "Step: 76\n",
      "x_prev: [0.84098936 0.84098936]\n",
      "f(x): 0.6697961240504984\n",
      "Gradient:[0.83877767 0.83877767]\n",
      "\n",
      "x-x_prev:9.783169752046803e-06 < eps: 1e-06\n",
      "\n",
      "Step: 77\n",
      "x_prev: [0.83877767 0.83877767]\n",
      "f(x): 0.6689739340480579\n",
      "Gradient:[0.83656319 0.83656319]\n",
      "\n",
      "x-x_prev:9.80782647641873e-06 < eps: 1e-06\n",
      "\n",
      "Step: 78\n",
      "x_prev: [0.83656319 0.83656319]\n",
      "f(x): 0.6681518470087341\n",
      "Gradient:[0.83434594 0.83434594]\n",
      "\n",
      "x-x_prev:9.832391253464282e-06 < eps: 1e-06\n",
      "\n",
      "Step: 79\n",
      "x_prev: [0.83434594 0.83434594]\n",
      "f(x): 0.6673298842731654\n",
      "Gradient:[0.83212593 0.83212593]\n",
      "\n",
      "x-x_prev:9.856863032927407e-06 < eps: 1e-06\n",
      "\n",
      "Step: 80\n",
      "x_prev: [0.83212593 0.83212593]\n",
      "f(x): 0.6665080672147249\n",
      "Gradient:[0.82990318 0.82990318]\n",
      "\n",
      "x-x_prev:9.881240775445838e-06 < eps: 1e-06\n",
      "\n",
      "Step: 81\n",
      "x_prev: [0.82990318 0.82990318]\n",
      "f(x): 0.6656864172373257\n",
      "Gradient:[0.8276777 0.8276777]\n",
      "\n",
      "x-x_prev:9.905523452745719e-06 < eps: 1e-06\n",
      "\n",
      "Step: 82\n",
      "x_prev: [0.8276777 0.8276777]\n",
      "f(x): 0.6648649557732218\n",
      "Gradient:[0.82544951 0.82544951]\n",
      "\n",
      "x-x_prev:9.929710047827053e-06 < eps: 1e-06\n",
      "\n",
      "Step: 83\n",
      "x_prev: [0.82544951 0.82544951]\n",
      "f(x): 0.6640437042808056\n",
      "Gradient:[0.82321861 0.82321861]\n",
      "\n",
      "x-x_prev:9.95379955514976e-06 < eps: 1e-06\n",
      "\n",
      "Step: 84\n",
      "x_prev: [0.82321861 0.82321861]\n",
      "f(x): 0.6632226842424012\n",
      "Gradient:[0.82098503 0.82098503]\n",
      "\n",
      "x-x_prev:9.977790980815385e-06 < eps: 1e-06\n",
      "\n",
      "Step: 85\n",
      "x_prev: [0.82098503 0.82098503]\n",
      "f(x): 0.6624019171620541\n",
      "Gradient:[0.81874877 0.81874877]\n",
      "\n",
      "x-x_prev:1.0001683342748368e-05 < eps: 1e-06\n",
      "\n",
      "Step: 86\n",
      "x_prev: [0.81874877 0.81874877]\n",
      "f(x): 0.6615814245633207\n",
      "Gradient:[0.81650986 0.81650986]\n",
      "\n",
      "x-x_prev:1.0025475670867911e-05 < eps: 1e-06\n",
      "\n",
      "Step: 87\n",
      "x_prev: [0.81650986 0.81650986]\n",
      "f(x): 0.6607612279870542\n",
      "Gradient:[0.8142683 0.8142683]\n",
      "\n",
      "x-x_prev:1.0049167007264277e-05 < eps: 1e-06\n",
      "\n",
      "Step: 88\n",
      "x_prev: [0.8142683 0.8142683]\n",
      "f(x): 0.65994134898919\n",
      "Gradient:[0.81202411 0.81202411]\n",
      "\n",
      "x-x_prev:1.0072756406368575e-05 < eps: 1e-06\n",
      "\n",
      "Step: 89\n",
      "x_prev: [0.81202411 0.81202411]\n",
      "f(x): 0.6591218091385312\n",
      "Gradient:[0.80977731 0.80977731]\n",
      "\n",
      "x-x_prev:1.009624293511399e-05 < eps: 1e-06\n",
      "\n",
      "Step: 90\n",
      "x_prev: [0.80977731 0.80977731]\n",
      "f(x): 0.6583026300145345\n",
      "Gradient:[0.8075279 0.8075279]\n",
      "\n",
      "x-x_prev:1.0119625673102337e-05 < eps: 1e-06\n",
      "\n",
      "Step: 91\n",
      "x_prev: [0.8075279 0.8075279]\n",
      "f(x): 0.657483833205095\n",
      "Gradient:[0.80527592 0.80527592]\n",
      "\n",
      "x-x_prev:1.0142903712762954e-05 < eps: 1e-06\n",
      "\n",
      "Step: 92\n",
      "x_prev: [0.80527592 0.80527592]\n",
      "f(x): 0.6566654403043344\n",
      "Gradient:[0.80302136 0.80302136]\n",
      "\n",
      "x-x_prev:1.0166076159504918e-05 < eps: 1e-06\n",
      "\n",
      "Step: 93\n",
      "x_prev: [0.80302136 0.80302136]\n",
      "f(x): 0.6558474729103911\n",
      "Gradient:[0.80076424 0.80076424]\n",
      "\n",
      "x-x_prev:1.0189142131873472e-05 < eps: 1e-06\n",
      "\n",
      "Step: 94\n",
      "x_prev: [0.80076424 0.80076424]\n",
      "f(x): 0.6550299526232117\n",
      "Gradient:[0.79850458 0.79850458]\n",
      "\n",
      "x-x_prev:1.0212100761692684e-05 < eps: 1e-06\n",
      "\n",
      "Step: 95\n",
      "x_prev: [0.79850458 0.79850458]\n",
      "f(x): 0.6542129010423468\n",
      "Gradient:[0.7962424 0.7962424]\n",
      "\n",
      "x-x_prev:1.023495119421527e-05 < eps: 1e-06\n",
      "\n",
      "Step: 96\n",
      "x_prev: [0.7962424 0.7962424]\n",
      "f(x): 0.6533963397647494\n",
      "Gradient:[0.7939777 0.7939777]\n",
      "\n",
      "x-x_prev:1.0257692588260557e-05 < eps: 1e-06\n",
      "\n",
      "Step: 97\n",
      "x_prev: [0.7939777 0.7939777]\n",
      "f(x): 0.6525802903825803\n",
      "Gradient:[0.79171051 0.79171051]\n",
      "\n",
      "x-x_prev:1.0280324116350525e-05 < eps: 1e-06\n",
      "\n",
      "Step: 98\n",
      "x_prev: [0.79171051 0.79171051]\n",
      "f(x): 0.6517647744810154\n",
      "Gradient:[0.78944084 0.78944084]\n",
      "\n",
      "x-x_prev:1.0302844964844935e-05 < eps: 1e-06\n",
      "\n",
      "Step: 99\n",
      "x_prev: [0.78944084 0.78944084]\n",
      "f(x): 0.6509498136360605\n",
      "Gradient:[0.7871687 0.7871687]\n",
      "\n",
      "x-x_prev:1.0325254334068444e-05 < eps: 1e-06\n",
      "\n",
      "Step: 100\n",
      "x_prev: [0.7871687 0.7871687]\n",
      "f(x): 0.6501354294123725\n",
      "Gradient:[0.7848941 0.7848941]\n",
      "\n",
      "x-x_prev:1.0347551438437745e-05 < eps: 1e-06\n",
      "\n",
      "Step: 101\n",
      "x_prev: [0.7848941 0.7848941]\n",
      "f(x): 0.6493216433610859\n",
      "Gradient:[0.78261707 0.78261707]\n",
      "\n",
      "x-x_prev:1.0369735506581633e-05 < eps: 1e-06\n",
      "\n",
      "Step: 102\n",
      "x_prev: [0.78261707 0.78261707]\n",
      "f(x): 0.6485084770176488\n",
      "Gradient:[0.78033762 0.78033762]\n",
      "\n",
      "x-x_prev:1.039180578145698e-05 < eps: 1e-06\n",
      "\n",
      "Step: 103\n",
      "x_prev: [0.78033762 0.78033762]\n",
      "f(x): 0.6476959518996649\n",
      "Gradient:[0.77805576 0.77805576]\n",
      "\n",
      "x-x_prev:1.0413761520464641e-05 < eps: 1e-06\n",
      "\n",
      "Step: 104\n",
      "x_prev: [0.77805576 0.77805576]\n",
      "f(x): 0.6468840895047454\n",
      "Gradient:[0.77577151 0.77577151]\n",
      "\n",
      "x-x_prev:1.043560199555616e-05 < eps: 1e-06\n",
      "\n",
      "Step: 105\n",
      "x_prev: [0.77577151 0.77577151]\n",
      "f(x): 0.6460729113083693\n",
      "Gradient:[0.77348488 0.77348488]\n",
      "\n",
      "x-x_prev:1.0457326493341354e-05 < eps: 1e-06\n",
      "\n",
      "Step: 106\n",
      "x_prev: [0.77348488 0.77348488]\n",
      "f(x): 0.6452624387617543\n",
      "Gradient:[0.77119589 0.77119589]\n",
      "\n",
      "x-x_prev:1.0478934315186623e-05 < eps: 1e-06\n",
      "\n",
      "Step: 107\n",
      "x_prev: [0.77119589 0.77119589]\n",
      "f(x): 0.6444526932897365\n",
      "Gradient:[0.76890456 0.76890456]\n",
      "\n",
      "x-x_prev:1.0500424777310026e-05 < eps: 1e-06\n",
      "\n",
      "Step: 108\n",
      "x_prev: [0.76890456 0.76890456]\n",
      "f(x): 0.6436436962886628\n",
      "Gradient:[0.7666109 0.7666109]\n",
      "\n",
      "x-x_prev:1.0521797210881234e-05 < eps: 1e-06\n",
      "\n",
      "Step: 109\n",
      "x_prev: [0.7666109 0.7666109]\n",
      "f(x): 0.6428354691242929\n",
      "Gradient:[0.76431492 0.76431492]\n",
      "\n",
      "x-x_prev:1.0543050962099912e-05 < eps: 1e-06\n",
      "\n",
      "Step: 110\n",
      "x_prev: [0.76431492 0.76431492]\n",
      "f(x): 0.6420280331297149\n",
      "Gradient:[0.76201663 0.76201663]\n",
      "\n",
      "x-x_prev:1.0564185392287059e-05 < eps: 1e-06\n",
      "\n",
      "Step: 111\n",
      "x_prev: [0.76201663 0.76201663]\n",
      "f(x): 0.6412214096032715\n",
      "Gradient:[0.75971607 0.75971607]\n",
      "\n",
      "x-x_prev:1.058519987796386e-05 < eps: 1e-06\n",
      "\n",
      "Step: 112\n",
      "x_prev: [0.75971607 0.75971607]\n",
      "f(x): 0.6404156198065007\n",
      "Gradient:[0.75741324 0.75741324]\n",
      "\n",
      "x-x_prev:1.060609381092209e-05 < eps: 1e-06\n",
      "\n",
      "Step: 113\n",
      "x_prev: [0.75741324 0.75741324]\n",
      "f(x): 0.6396106849620902\n",
      "Gradient:[0.75510815 0.75510815]\n",
      "\n",
      "x-x_prev:1.062686659830337e-05 < eps: 1e-06\n",
      "\n",
      "Step: 114\n",
      "x_prev: [0.75510815 0.75510815]\n",
      "f(x): 0.6388066262518438\n",
      "Gradient:[0.75280082 0.75280082]\n",
      "\n",
      "x-x_prev:1.0647517662658678e-05 < eps: 1e-06\n",
      "\n",
      "Step: 115\n",
      "x_prev: [0.75280082 0.75280082]\n",
      "f(x): 0.638003464814665\n",
      "Gradient:[0.75049127 0.75049127]\n",
      "\n",
      "x-x_prev:1.0668046442014671e-05 < eps: 1e-06\n",
      "\n",
      "Step: 116\n",
      "x_prev: [0.75049127 0.75049127]\n",
      "f(x): 0.6372012217445535\n",
      "Gradient:[0.74817951 0.74817951]\n",
      "\n",
      "x-x_prev:1.0688452389935532e-05 < eps: 1e-06\n",
      "\n",
      "Step: 117\n",
      "x_prev: [0.74817951 0.74817951]\n",
      "f(x): 0.6363999180886192\n",
      "Gradient:[0.74586556 0.74586556]\n",
      "\n",
      "x-x_prev:1.0708734975568035e-05 < eps: 1e-06\n",
      "\n",
      "Step: 118\n",
      "x_prev: [0.74586556 0.74586556]\n",
      "f(x): 0.6355995748451099\n",
      "Gradient:[0.74354943 0.74354943]\n",
      "\n",
      "x-x_prev:1.0728893683700552e-05 < eps: 1e-06\n",
      "\n",
      "Step: 119\n",
      "x_prev: [0.74354943 0.74354943]\n",
      "f(x): 0.6348002129614576\n",
      "Gradient:[0.74123114 0.74123114]\n",
      "\n",
      "x-x_prev:1.0748928014803181e-05 < eps: 1e-06\n",
      "\n",
      "Step: 120\n",
      "x_prev: [0.74123114 0.74123114]\n",
      "f(x): 0.6340018533323415\n",
      "Gradient:[0.73891071 0.73891071]\n",
      "\n",
      "x-x_prev:1.076883748507255e-05 < eps: 1e-06\n",
      "\n",
      "Step: 121\n",
      "x_prev: [0.73891071 0.73891071]\n",
      "f(x): 0.6332045167977665\n",
      "Gradient:[0.73658814 0.73658814]\n",
      "\n",
      "x-x_prev:1.0788621626461788e-05 < eps: 1e-06\n",
      "\n",
      "Step: 122\n",
      "x_prev: [0.73658814 0.73658814]\n",
      "f(x): 0.6324082241411627\n",
      "Gradient:[0.73426346 0.73426346]\n",
      "\n",
      "x-x_prev:1.0808279986724442e-05 < eps: 1e-06\n",
      "\n",
      "Step: 123\n",
      "x_prev: [0.73426346 0.73426346]\n",
      "f(x): 0.6316129960875007\n",
      "Gradient:[0.73193668 0.73193668]\n",
      "\n",
      "x-x_prev:1.0827812129432168e-05 < eps: 1e-06\n",
      "\n",
      "Step: 124\n",
      "x_prev: [0.73193668 0.73193668]\n",
      "f(x): 0.6308188533014271\n",
      "Gradient:[0.72960782 0.72960782]\n",
      "\n",
      "x-x_prev:1.0847217634005326e-05 < eps: 1e-06\n",
      "\n",
      "Step: 125\n",
      "x_prev: [0.72960782 0.72960782]\n",
      "f(x): 0.630025816385418\n",
      "Gradient:[0.72727688 0.72727688]\n",
      "\n",
      "x-x_prev:1.0866496095727626e-05 < eps: 1e-06\n",
      "\n",
      "Step: 126\n",
      "x_prev: [0.72727688 0.72727688]\n",
      "f(x): 0.6292339058779545\n",
      "Gradient:[0.7249439 0.7249439]\n",
      "\n",
      "x-x_prev:1.0885647125768504e-05 < eps: 1e-06\n",
      "\n",
      "Step: 127\n",
      "x_prev: [0.7249439 0.7249439]\n",
      "f(x): 0.6284431422517148\n",
      "Gradient:[0.72260888 0.72260888]\n",
      "\n",
      "x-x_prev:1.090467035118744e-05 < eps: 1e-06\n",
      "\n",
      "Step: 128\n",
      "x_prev: [0.72260888 0.72260888]\n",
      "f(x): 0.6276535459117902\n",
      "Gradient:[0.72027183 0.72027183]\n",
      "\n",
      "x-x_prev:1.0923565414946022e-05 < eps: 1e-06\n",
      "\n",
      "Step: 129\n",
      "x_prev: [0.72027183 0.72027183]\n",
      "f(x): 0.6268651371939195\n",
      "Gradient:[0.71793278 0.71793278]\n",
      "\n",
      "x-x_prev:1.094233197590913e-05 < eps: 1e-06\n",
      "\n",
      "Step: 130\n",
      "x_prev: [0.71793278 0.71793278]\n",
      "f(x): 0.6260779363627453\n",
      "Gradient:[0.71559173 0.71559173]\n",
      "\n",
      "x-x_prev:1.0960969708841442e-05 < eps: 1e-06\n",
      "\n",
      "Step: 131\n",
      "x_prev: [0.71559173 0.71559173]\n",
      "f(x): 0.6252919636100931\n",
      "Gradient:[0.71324871 0.71324871]\n",
      "\n",
      "x-x_prev:1.097947830440549e-05 < eps: 1e-06\n",
      "\n",
      "Step: 132\n",
      "x_prev: [0.71324871 0.71324871]\n",
      "f(x): 0.6245072390532704\n",
      "Gradient:[0.71090373 0.71090373]\n",
      "\n",
      "x-x_prev:1.099785746915086e-05 < eps: 1e-06\n",
      "\n",
      "Step: 133\n",
      "x_prev: [0.71090373 0.71090373]\n",
      "f(x): 0.6237237827333894\n",
      "Gradient:[0.70855681 0.70855681]\n",
      "\n",
      "x-x_prev:1.1016106925500798e-05 < eps: 1e-06\n",
      "\n",
      "Step: 134\n",
      "x_prev: [0.70855681 0.70855681]\n",
      "f(x): 0.622941614613712\n",
      "Gradient:[0.70620796 0.70620796]\n",
      "\n",
      "x-x_prev:1.1034226411734086e-05 < eps: 1e-06\n",
      "\n",
      "Step: 135\n",
      "x_prev: [0.70620796 0.70620796]\n",
      "f(x): 0.6221607545780176\n",
      "Gradient:[0.70385719 0.70385719]\n",
      "\n",
      "x-x_prev:1.1052215681961186e-05 < eps: 1e-06\n",
      "\n",
      "Step: 136\n",
      "x_prev: [0.70385719 0.70385719]\n",
      "f(x): 0.6213812224289941\n",
      "Gradient:[0.70150452 0.70150452]\n",
      "\n",
      "x-x_prev:1.107007450610297e-05 < eps: 1e-06\n",
      "\n",
      "Step: 137\n",
      "x_prev: [0.70150452 0.70150452]\n",
      "f(x): 0.6206030378866533\n",
      "Gradient:[0.69914997 0.69914997]\n",
      "\n",
      "x-x_prev:1.1087802669857434e-05 < eps: 1e-06\n",
      "\n",
      "Step: 138\n",
      "x_prev: [0.69914997 0.69914997]\n",
      "f(x): 0.6198262205867693\n",
      "Gradient:[0.69679356 0.69679356]\n",
      "\n",
      "x-x_prev:1.1105399974667966e-05 < eps: 1e-06\n",
      "\n",
      "Step: 139\n",
      "x_prev: [0.69679356 0.69679356]\n",
      "f(x): 0.619050790079341\n",
      "Gradient:[0.69443529 0.69443529]\n",
      "\n",
      "x-x_prev:1.1122866237684804e-05 < eps: 1e-06\n",
      "\n",
      "Step: 140\n",
      "x_prev: [0.69443529 0.69443529]\n",
      "f(x): 0.6182767658270801\n",
      "Gradient:[0.69207518 0.69207518]\n",
      "\n",
      "x-x_prev:1.1140201291721782e-05 < eps: 1e-06\n",
      "\n",
      "Step: 141\n",
      "x_prev: [0.69207518 0.69207518]\n",
      "f(x): 0.6175041672039226\n",
      "Gradient:[0.68971325 0.68971325]\n",
      "\n",
      "x-x_prev:1.1157404985216744e-05 < eps: 1e-06\n",
      "\n",
      "Step: 142\n",
      "x_prev: [0.68971325 0.68971325]\n",
      "f(x): 0.6167330134935659\n",
      "Gradient:[0.68734952 0.68734952]\n",
      "\n",
      "x-x_prev:1.1174477182174656e-05 < eps: 1e-06\n",
      "\n",
      "Step: 143\n",
      "x_prev: [0.68734952 0.68734952]\n",
      "f(x): 0.6159633238880309\n",
      "Gradient:[0.684984 0.684984]\n",
      "\n",
      "x-x_prev:1.119141776212281e-05 < eps: 1e-06\n",
      "\n",
      "Step: 144\n",
      "x_prev: [0.684984 0.684984]\n",
      "f(x): 0.6151951174862503\n",
      "Gradient:[0.68261669 0.68261669]\n",
      "\n",
      "x-x_prev:1.1208226620046601e-05 < eps: 1e-06\n",
      "\n",
      "Step: 145\n",
      "x_prev: [0.68261669 0.68261669]\n",
      "f(x): 0.6144284132926809\n",
      "Gradient:[0.68024763 0.68024763]\n",
      "\n",
      "x-x_prev:1.1224903666335334e-05 < eps: 1e-06\n",
      "\n",
      "Step: 146\n",
      "x_prev: [0.68024763 0.68024763]\n",
      "f(x): 0.6136632302159434\n",
      "Gradient:[0.67787683 0.67787683]\n",
      "\n",
      "x-x_prev:1.1241448826716998e-05 < eps: 1e-06\n",
      "\n",
      "Step: 147\n",
      "x_prev: [0.67787683 0.67787683]\n",
      "f(x): 0.6128995870674873\n",
      "Gradient:[0.67550429 0.67550429]\n",
      "\n",
      "x-x_prev:1.1257862042188372e-05 < eps: 1e-06\n",
      "\n",
      "Step: 148\n",
      "x_prev: [0.67550429 0.67550429]\n",
      "f(x): 0.6121375025602829\n",
      "Gradient:[0.67313004 0.67313004]\n",
      "\n",
      "x-x_prev:1.12741432689478e-05 < eps: 1e-06\n",
      "\n",
      "Step: 149\n",
      "x_prev: [0.67313004 0.67313004]\n",
      "f(x): 0.6113769953075385\n",
      "Gradient:[0.67075409 0.67075409]\n",
      "\n",
      "x-x_prev:1.1290292478316955e-05 < eps: 1e-06\n",
      "\n",
      "Step: 150\n",
      "x_prev: [0.67075409 0.67075409]\n",
      "f(x): 0.6106180838214453\n",
      "Gradient:[0.66837645 0.66837645]\n",
      "\n",
      "x-x_prev:1.1306309656669563e-05 < eps: 1e-06\n",
      "\n",
      "Step: 151\n",
      "x_prev: [0.66837645 0.66837645]\n",
      "f(x): 0.6098607865119487\n",
      "Gradient:[0.66599714 0.66599714]\n",
      "\n",
      "x-x_prev:1.1322194805341706e-05 < eps: 1e-06\n",
      "\n",
      "Step: 152\n",
      "x_prev: [0.66599714 0.66599714]\n",
      "f(x): 0.6091051216855463\n",
      "Gradient:[0.66361618 0.66361618]\n",
      "\n",
      "x-x_prev:1.1337947940554386e-05 < eps: 1e-06\n",
      "\n",
      "Step: 153\n",
      "x_prev: [0.66361618 0.66361618]\n",
      "f(x): 0.6083511075441131\n",
      "Gradient:[0.66123358 0.66123358]\n",
      "\n",
      "x-x_prev:1.1353569093325097e-05 < eps: 1e-06\n",
      "\n",
      "Step: 154\n",
      "x_prev: [0.66123358 0.66123358]\n",
      "f(x): 0.6075987621837536\n",
      "Gradient:[0.65884936 0.65884936]\n",
      "\n",
      "x-x_prev:1.1369058309372658e-05 < eps: 1e-06\n",
      "\n",
      "Step: 155\n",
      "x_prev: [0.65884936 0.65884936]\n",
      "f(x): 0.6068481035936818\n",
      "Gradient:[0.65646352 0.65646352]\n",
      "\n",
      "x-x_prev:1.1384415649029038e-05 < eps: 1e-06\n",
      "\n",
      "Step: 156\n",
      "x_prev: [0.65646352 0.65646352]\n",
      "f(x): 0.606099149655128\n",
      "Gradient:[0.65407609 0.65407609]\n",
      "\n",
      "x-x_prev:1.1399641187138117e-05 < eps: 1e-06\n",
      "\n",
      "Step: 157\n",
      "x_prev: [0.65407609 0.65407609]\n",
      "f(x): 0.6053519181402726\n",
      "Gradient:[0.65168708 0.65168708]\n",
      "\n",
      "x-x_prev:1.1414735012958305e-05 < eps: 1e-06\n",
      "\n",
      "Step: 158\n",
      "x_prev: [0.65168708 0.65168708]\n",
      "f(x): 0.6046064267112088\n",
      "Gradient:[0.64929651 0.64929651]\n",
      "\n",
      "x-x_prev:1.1429697230058472e-05 < eps: 1e-06\n",
      "\n",
      "Step: 159\n",
      "x_prev: [0.64929651 0.64929651]\n",
      "f(x): 0.6038626929189319\n",
      "Gradient:[0.64690438 0.64690438]\n",
      "\n",
      "x-x_prev:1.1444527956213523e-05 < eps: 1e-06\n",
      "\n",
      "Step: 160\n",
      "x_prev: [0.64690438 0.64690438]\n",
      "f(x): 0.6031207342023558\n",
      "Gradient:[0.64451072 0.64451072]\n",
      "\n",
      "x-x_prev:1.1459227323293303e-05 < eps: 1e-06\n",
      "\n",
      "Step: 161\n",
      "x_prev: [0.64451072 0.64451072]\n",
      "f(x): 0.6023805678873586\n",
      "Gradient:[0.64211554 0.64211554]\n",
      "\n",
      "x-x_prev:1.147379547715438e-05 < eps: 1e-06\n",
      "\n",
      "Step: 162\n",
      "x_prev: [0.64211554 0.64211554]\n",
      "f(x): 0.6016422111858545\n",
      "Gradient:[0.63971885 0.63971885]\n",
      "\n",
      "x-x_prev:1.1488232577524118e-05 < eps: 1e-06\n",
      "\n",
      "Step: 163\n",
      "x_prev: [0.63971885 0.63971885]\n",
      "f(x): 0.600905681194895\n",
      "Gradient:[0.63732067 0.63732067]\n",
      "\n",
      "x-x_prev:1.1502538797882333e-05 < eps: 1e-06\n",
      "\n",
      "Step: 164\n",
      "x_prev: [0.63732067 0.63732067]\n",
      "f(x): 0.600170994895796\n",
      "Gradient:[0.63492101 0.63492101]\n",
      "\n",
      "x-x_prev:1.1516714325346993e-05 < eps: 1e-06\n",
      "\n",
      "Step: 165\n",
      "x_prev: [0.63492101 0.63492101]\n",
      "f(x): 0.599438169153294\n",
      "Gradient:[0.63251989 0.63251989]\n",
      "\n",
      "x-x_prev:1.1530759360545814e-05 < eps: 1e-06\n",
      "\n",
      "Step: 166\n",
      "x_prev: [0.63251989 0.63251989]\n",
      "f(x): 0.5987072207147304\n",
      "Gradient:[0.63011732 0.63011732]\n",
      "\n",
      "x-x_prev:1.1544674117499393e-05 < eps: 1e-06\n",
      "\n",
      "Step: 167\n",
      "x_prev: [0.63011732 0.63011732]\n",
      "f(x): 0.5979781662092618\n",
      "Gradient:[0.62771332 0.62771332]\n",
      "\n",
      "x-x_prev:1.1558458823489209e-05 < eps: 1e-06\n",
      "\n",
      "Step: 168\n",
      "x_prev: [0.62771332 0.62771332]\n",
      "f(x): 0.5972510221470998\n",
      "Gradient:[0.6253079 0.6253079]\n",
      "\n",
      "x-x_prev:1.1572113718928665e-05 < eps: 1e-06\n",
      "\n",
      "Step: 169\n",
      "x_prev: [0.6253079 0.6253079]\n",
      "f(x): 0.5965258049187782\n",
      "Gradient:[0.62290107 0.62290107]\n",
      "\n",
      "x-x_prev:1.1585639057236101e-05 < eps: 1e-06\n",
      "\n",
      "Step: 170\n",
      "x_prev: [0.62290107 0.62290107]\n",
      "f(x): 0.5958025307944461\n",
      "Gradient:[0.62049285 0.62049285]\n",
      "\n",
      "x-x_prev:1.1599035104698083e-05 < eps: 1e-06\n",
      "\n",
      "Step: 171\n",
      "x_prev: [0.62049285 0.62049285]\n",
      "f(x): 0.5950812159231919\n",
      "Gradient:[0.61808326 0.61808326]\n",
      "\n",
      "x-x_prev:1.1612302140333631e-05 < eps: 1e-06\n",
      "\n",
      "Step: 172\n",
      "x_prev: [0.61808326 0.61808326]\n",
      "f(x): 0.5943618763323915\n",
      "Gradient:[0.6156723 0.6156723]\n",
      "\n",
      "x-x_prev:1.1625440455760512e-05 < eps: 1e-06\n",
      "\n",
      "Step: 173\n",
      "x_prev: [0.6156723 0.6156723]\n",
      "f(x): 0.5936445279270858\n",
      "Gradient:[0.61325999 0.61325999]\n",
      "\n",
      "x-x_prev:1.1638450355048624e-05 < eps: 1e-06\n",
      "\n",
      "Step: 174\n",
      "x_prev: [0.61325999 0.61325999]\n",
      "f(x): 0.5929291864893859\n",
      "Gradient:[0.61084635 0.61084635]\n",
      "\n",
      "x-x_prev:1.1651332154586168e-05 < eps: 1e-06\n",
      "\n",
      "Step: 175\n",
      "x_prev: [0.61084635 0.61084635]\n",
      "f(x): 0.592215867677904\n",
      "Gradient:[0.60843139 0.60843139]\n",
      "\n",
      "x-x_prev:1.1664086182932961e-05 < eps: 1e-06\n",
      "\n",
      "Step: 176\n",
      "x_prev: [0.60843139 0.60843139]\n",
      "f(x): 0.5915045870272131\n",
      "Gradient:[0.60601512 0.60601512]\n",
      "\n",
      "x-x_prev:1.1676712780675884e-05 < eps: 1e-06\n",
      "\n",
      "Step: 177\n",
      "x_prev: [0.60601512 0.60601512]\n",
      "f(x): 0.5907953599473327\n",
      "Gradient:[0.60359756 0.60359756]\n",
      "\n",
      "x-x_prev:1.1689212300277899e-05 < eps: 1e-06\n",
      "\n",
      "Step: 178\n",
      "x_prev: [0.60359756 0.60359756]\n",
      "f(x): 0.5900882017232427\n",
      "Gradient:[0.60117871 0.60117871]\n",
      "\n",
      "x-x_prev:1.1701585105938933e-05 < eps: 1e-06\n",
      "\n",
      "Step: 179\n",
      "x_prev: [0.60117871 0.60117871]\n",
      "f(x): 0.5893831275144222\n",
      "Gradient:[0.59875861 0.59875861]\n",
      "\n",
      "x-x_prev:1.1713831573436419e-05 < eps: 1e-06\n",
      "\n",
      "Step: 180\n",
      "x_prev: [0.59875861 0.59875861]\n",
      "f(x): 0.5886801523544176\n",
      "Gradient:[0.59633725 0.59633725]\n",
      "\n",
      "x-x_prev:1.1725952089977728e-05 < eps: 1e-06\n",
      "\n",
      "Step: 181\n",
      "x_prev: [0.59633725 0.59633725]\n",
      "f(x): 0.587979291150435\n",
      "Gradient:[0.59391465 0.59391465]\n",
      "\n",
      "x-x_prev:1.1737947054050585e-05 < eps: 1e-06\n",
      "\n",
      "Step: 182\n",
      "x_prev: [0.59391465 0.59391465]\n",
      "f(x): 0.5872805586829603\n",
      "Gradient:[0.59149083 0.59149083]\n",
      "\n",
      "x-x_prev:1.1749816875260694e-05 < eps: 1e-06\n",
      "\n",
      "Step: 183\n",
      "x_prev: [0.59149083 0.59149083]\n",
      "f(x): 0.5865839696054056\n",
      "Gradient:[0.5890658 0.5890658]\n",
      "\n",
      "x-x_prev:1.1761561974183509e-05 < eps: 1e-06\n",
      "\n",
      "Step: 184\n",
      "x_prev: [0.5890658 0.5890658]\n",
      "f(x): 0.585889538443781\n",
      "Gradient:[0.58663957 0.58663957]\n",
      "\n",
      "x-x_prev:1.1773182782203261e-05 < eps: 1e-06\n",
      "\n",
      "Step: 185\n",
      "x_prev: [0.58663957 0.58663957]\n",
      "f(x): 0.5851972795963933\n",
      "Gradient:[0.58421216 0.58421216]\n",
      "\n",
      "x-x_prev:1.1784679741355435e-05 < eps: 1e-06\n",
      "\n",
      "Step: 186\n",
      "x_prev: [0.58421216 0.58421216]\n",
      "f(x): 0.5845072073335695\n",
      "Gradient:[0.58178357 0.58178357]\n",
      "\n",
      "x-x_prev:1.1796053304168366e-05 < eps: 1e-06\n",
      "\n",
      "Step: 187\n",
      "x_prev: [0.58178357 0.58178357]\n",
      "f(x): 0.5838193357974067\n",
      "Gradient:[0.57935383 0.57935383]\n",
      "\n",
      "x-x_prev:1.1807303933500784e-05 < eps: 1e-06\n",
      "\n",
      "Step: 188\n",
      "x_prev: [0.57935383 0.57935383]\n",
      "f(x): 0.5831336790015474\n",
      "Gradient:[0.57692294 0.57692294]\n",
      "\n",
      "x-x_prev:1.1818432102386101e-05 < eps: 1e-06\n",
      "\n",
      "Step: 189\n",
      "x_prev: [0.57692294 0.57692294]\n",
      "f(x): 0.582450250830979\n",
      "Gradient:[0.57449092 0.57449092]\n",
      "\n",
      "x-x_prev:1.1829438293860808e-05 < eps: 1e-06\n",
      "\n",
      "Step: 190\n",
      "x_prev: [0.57449092 0.57449092]\n",
      "f(x): 0.5817690650418601\n",
      "Gradient:[0.57205778 0.57205778]\n",
      "\n",
      "x-x_prev:1.1840323000810438e-05 < eps: 1e-06\n",
      "\n",
      "Step: 191\n",
      "x_prev: [0.57205778 0.57205778]\n",
      "f(x): 0.5810901352613699\n",
      "Gradient:[0.56962354 0.56962354]\n",
      "\n",
      "x-x_prev:1.1851086725800773e-05 < eps: 1e-06\n",
      "\n",
      "Step: 192\n",
      "x_prev: [0.56962354 0.56962354]\n",
      "f(x): 0.5804134749875832\n",
      "Gradient:[0.5671882 0.5671882]\n",
      "\n",
      "x-x_prev:1.1861729980911558e-05 < eps: 1e-06\n",
      "\n",
      "Step: 193\n",
      "x_prev: [0.5671882 0.5671882]\n",
      "f(x): 0.5797390975893696\n",
      "Gradient:[0.56475178 0.56475178]\n",
      "\n",
      "x-x_prev:1.1872253287578168e-05 < eps: 1e-06\n",
      "\n",
      "Step: 194\n",
      "x_prev: [0.56475178 0.56475178]\n",
      "f(x): 0.5790670163063162\n",
      "Gradient:[0.5623143 0.5623143]\n",
      "\n",
      "x-x_prev:1.188265717641855e-05 < eps: 1e-06\n",
      "\n",
      "Step: 195\n",
      "x_prev: [0.5623143 0.5623143]\n",
      "f(x): 0.5783972442486757\n",
      "Gradient:[0.55987576 0.55987576]\n",
      "\n",
      "x-x_prev:1.1892942187068154e-05 < eps: 1e-06\n",
      "\n",
      "Step: 196\n",
      "x_prev: [0.55987576 0.55987576]\n",
      "f(x): 0.5777297943973362\n",
      "Gradient:[0.55743618 0.55743618]\n",
      "\n",
      "x-x_prev:1.1903108868016401e-05 < eps: 1e-06\n",
      "\n",
      "Step: 197\n",
      "x_prev: [0.55743618 0.55743618]\n",
      "f(x): 0.5770646796038165\n",
      "Gradient:[0.55499557 0.55499557]\n",
      "\n",
      "x-x_prev:1.1913157776436062e-05 < eps: 1e-06\n",
      "\n",
      "Step: 198\n",
      "x_prev: [0.55499557 0.55499557]\n",
      "f(x): 0.5764019125902832\n",
      "Gradient:[0.55255394 0.55255394]\n",
      "\n",
      "x-x_prev:1.1923089478015276e-05 < eps: 1e-06\n",
      "\n",
      "Step: 199\n",
      "x_prev: [0.55255394 0.55255394]\n",
      "f(x): 0.5757415059495906\n",
      "Gradient:[0.55011131 0.55011131]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_min = gradient_descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "7.38905609893065\n",
      "3.680855854801923e-272\n"
     ]
    }
   ],
   "source": [
    "from math import exp\n",
    "\n",
    "print(exp(-2500))\n",
    "print(np.exp(2))\n",
    "print(np.e**-(25*25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YyKaCWuJpP93"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Домашнее задание</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jrjiC9mUpP93"
   },
   "source": [
    "1). (только для тех, кто раньше брал производные) Вычислите производную функции $f(x)=\\frac{1}{x}$ по определению и сравните с производной степенной функции в общем случае;  \n",
    "2). Найдите производную функции $Cf(x)$, где С - число;  \n",
    "3). Найдите производные функций:  \n",
    "\n",
    "$$f(x)=x^3+3\\sqrt{x}-e^x$$\n",
    "\n",
    "$$f(x)=\\frac{x^2-1}{x^2+1}$$\n",
    "\n",
    "$$\\sigma(x)=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "$$L(y, \\hat{y}) = (y-\\hat{y})^2$$  \n",
    "\n",
    "4). Напишите формулу и код для градиентного спуска для функции:  \n",
    "$$f(w, x) = \\frac{1}{1 + e^{-wx}}$$  \n",
    "\n",
    "То есть по аналогии с примером 2 вычислите частные производные по $w$ и по $x$ и запишите формулу векторно (см. пример 2)\n",
    "\n",
    "В задаче 3 производную нужно брать по $\\hat{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxDBOB04pP93"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Полезные ссылки</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nm0VC825pP95"
   },
   "source": [
    "0). Прикольный сайт с рисунками путём задания кривых уравнениями и функциями:  \n",
    "\n",
    "https://www.desmos.com/calculator/jwshvscdzb\n",
    "\n",
    "***Производные:***\n",
    "\n",
    "1). Про то, как брать частные производные:  \n",
    "\n",
    "http://www.mathprofi.ru/chastnye_proizvodnye_primery.html\n",
    "\n",
    "2). Сайт на английском, но там много видеоуроков и задач по производным:  \n",
    "\n",
    "https://www.khanacademy.org/math/differential-calculus/derivative-intro-dc\n",
    "\n",
    "3). Задачи на частные производные:  \n",
    "\n",
    "http://ru.solverbook.com/primery-reshenij/primery-resheniya-chastnyx-proizvodnyx/  \n",
    "\n",
    "4). Ещё задачи на частные проивзодные:  \n",
    "\n",
    "https://xn--24-6kcaa2awqnc8dd.xn--p1ai/chastnye-proizvodnye-funkcii.html  \n",
    "\n",
    "5). Производные по матрицам:  \n",
    "\n",
    "http://nabatchikov.com/blog/view/matrix_der  \n",
    "\n",
    "***Градиентны спуск:***\n",
    "\n",
    "6). [Основная статья по градиентному спуску](http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D0%BE%D0%B3%D0%BE_%D1%81%D0%BF%D1%83%D1%81%D0%BA%D0%B0)\n",
    "\n",
    "7). Статья на Хабре про градиетный спуск для нейросетей:  \n",
    "\n",
    "https://habr.com/post/307312/  \n",
    "\n",
    "***Методы оптимизации в нейронных сетях:***\n",
    "\n",
    "8). Сайт с анимациями того, как сходятся алгоритмы градиентного спуска:\n",
    "www.denizyuret.com/2015/03/alec-radfords-animations-for.html\n",
    "\n",
    "9). Статья на Хабре про метопты (град. спуск) в нейронках:\n",
    "https://habr.com/post/318970/\n",
    "\n",
    "10). Ещё сайт (англ.) про метопты (град. спуск) в нейронках (очень подробно):\n",
    "http://ruder.io/optimizing-gradient-descent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[seminar]derivative_gradient.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
